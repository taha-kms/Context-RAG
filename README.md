Perfect âœ… â€” hereâ€™s a **complete, polished `README.md`** you can drop straight into your repo. Itâ€™s written to make your project **general-purpose**, not just news-related.

---

# ğŸ“š General-Purpose RAG Engine

A modular **Retrieval-Augmented Generation (RAG)** system built with:

* **ChromaDB** (vector store for embeddings)
* **OpenAI** (embeddings + LLMs)
* **Python** (clean, modular pipeline)

You can load documents in multiple formats (TXT, PDF, DOCX, MD, HTML, CSV), index them into a vector store, and ask natural-language questions. The system retrieves relevant chunks and generates concise, source-cited answers.

---

## ğŸš€ Features

* ğŸ“‚ Multi-format document loaders (TXT, PDF, DOCX, MD, HTML, CSV)
* âœ‚ï¸ Sentence-aware chunking with overlap
* ğŸ” Vector search with **OpenAI embeddings**
* âš–ï¸ Optional **hybrid retrieval** (BM25 + vectors)
* ğŸ’¬ Answer generation with **OpenAI GPT models**
* ğŸ“‘ Source citations with file + chunk reference
* ğŸ› ï¸ Modular design (easy to extend with other LLMs or vector stores)
* ğŸ–¥ï¸ CLI interface + optional API server (FastAPI)

---

## ğŸ“¦ Project Structure

```
rag-news/ (rename to your repo name)
â”œâ”€ .env.example            # Template for environment variables
â”œâ”€ requirements.txt        # Python dependencies
â”œâ”€ README.md               # This file
â”œâ”€ data/                   # Your source documents
â”‚   â”œâ”€ sample.txt
â”‚   â””â”€ ...
â”œâ”€ storage/                # ChromaDB persistent storage
â”œâ”€ main.py                 # CLI entrypoint
â””â”€ rag/                    # Core library
   â”œâ”€ config.py
   â”œâ”€ loaders.py
   â”œâ”€ chunking.py
   â”œâ”€ embeddings.py
   â”œâ”€ storage.py
   â”œâ”€ retriever.py
   â”œâ”€ generator.py
   â”œâ”€ io_utils.py
   â””â”€ pipeline.py
```

---

## âš™ï¸ Setup

### 1. Clone & Install

```bash
git clone https://github.com/taha-kms/context-hub.git
cd context-hub.git
pip install -r requirements.txt
```

### 2. Configure Environment

Copy the example file and add your API key:

```bash
cp .env.example .env
```

Edit `.env`:

```bash
OPENAI_API_KEY=sk-...
```

Optional: tweak chunk size, models, or retrieval settings.

---

## ğŸ“‚ Adding Documents

Place your files in the `data/` folder. Supported formats:

* `.txt`, `.md` â†’ plain text / markdown
* `.pdf` â†’ PDFs (text-based, not scanned images)
* `.docx` â†’ Word docs
* `.html` â†’ webpage exports
* `.csv` â†’ tabular data (flattened to text rows)

You can also organize them into subfolders (e.g. `data/news/`, `data/research/`).

---

## â–¶ï¸ Usage

### Rebuild index

Index all documents in `DATA_DIR`:

```bash
python main.py --reindex
```

### Ask a question

Query the index:

```bash
python main.py --question "What did the research paper conclude?"
```

Sample output:

```
Answer: The paper concludes that hybrid retrieval improves recall by 25%.
Sources: research_paper.pdf#chunk3
```

### Change retrieval depth

```bash
python main.py --question "What is X?" --n_results 8
```

---

## ğŸ§ª Example Domains

Try the engine with different kinds of knowledge:

* **News articles** â†’ â€œWhat happened in the Coinbase hack?â€
* **Research PDFs** â†’ â€œWhat is the main contribution of the paper?â€
* **Tech docs** â†’ â€œHow do I install package X?â€
* **Business reports** â†’ â€œWhat were Q2 earnings?â€
* **Policies/Legal** â†’ â€œWhat is the penalty for violation Y?â€
* **FAQs/CSV** â†’ â€œWhatâ€™s the refund policy?â€

---

## ğŸ”§ Advanced

* Hybrid retrieval: enable with `USE_HYBRID=true` in `.env`.
* Switch models: set `CHAT_MODEL` or `EMBED_MODEL` in `.env`.
* Run as API: add a small FastAPI wrapper (`api.py`) for `/ask` and `/reindex`.


---

## ğŸ¤ Contributing

Pull requests and issues welcome!
Ideas: add loaders, integrate new LLMs, improve evaluation.

